{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "import cam\n",
    "import util\n",
    "\n",
    "model_dir = Path('../saves/dam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/archive/CheXpert-v1.0-small.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-11cbfc5accfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchexpert_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/chexpert-fast-ai/models/util.py\u001b[0m in \u001b[0;36mchexpert_data_loader\u001b[0;34m(reparse, bs, use_hierarchy)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchexpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mfile_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchexpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchexpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchexpert\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/data/external.py\u001b[0m in \u001b[0;36mfile_extract\u001b[0;34m(fname, dest)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m   \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r:gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m     \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Unrecognized archive: {fname}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/storage/archive/CheXpert-v1.0-small.zip'"
     ]
    }
   ],
   "source": [
    "dls, labels = util.chexpert_data_loader(bs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_learner_base = util.ChexpertLearner(dls, densenet121, n_out=len(labels),\n",
    "                                        loss_func=BCEWithLogitsLossFlat(),\n",
    "                                        model_dir=Path('auc_maximization'),\n",
    "                                        metrics=[RocAucMulti(average=None),\n",
    "                                                 RocAucMulti(average='weighted')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_learner_base.find_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_learner_base.learn_model(use_saved=False, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the body of the trained base model\n",
    "torch.save(chexpert_learner_base.learn.model[0].state_dict(), model_dir/'chexpert_learner_base.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new densenet121 model\n",
    "model_dam = cnn_learner(dls, densenet121, n_out=len(labels), y_range=[0,1]).model\n",
    "\n",
    "# Replace the body of new model with the body from the base model\n",
    "model_dam[0].load_state_dict(torch.load(model_dir/'chexpert_learner_base.pth'))\n",
    "\n",
    "# Set all parameters to require grad\n",
    "for param in model_dam.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move model to cuda\n",
    "model_dam = model_dam.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libauc.losses import AUCMLoss\n",
    "from libauc.optimizers import PESG\n",
    "\n",
    "gamma = 500\n",
    "weight_decay = 0\n",
    "margin = 1.0\n",
    "\n",
    "loss_func = AUCMLoss()\n",
    "opt_func = PESG(model_dam, a=loss_func.a, b=loss_func.b, alpha=loss_func.alpha, gamma=gamma, margin=margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, dls, loss_func, opt_func, max_epoch=2, checkpoint=None):\n",
    "    # Load checkpoint if available\n",
    "    if checkpoint != None:\n",
    "        model_checkpoint = torch.load(checkpoint)\n",
    "        model.load_state_dict(model_checkpoint['state_dict'])\n",
    "        opt_func.load_state_dict(model_checkpoint['optimizer'])\n",
    "    \n",
    "    # Train model\n",
    "    max_auc = 0\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        model.train()    \n",
    "        for i, (data, targets) in enumerate(dls.train):\n",
    "            opt_func.zero_grad()\n",
    "            y_pred = model(data)\n",
    "            loss = loss_func(y_pred, targets)\n",
    "            loss.backward(retain_graph=True)\n",
    "            opt_func.step()\n",
    "\n",
    "            train_pred.append(y_pred.cpu().detach().numpy())\n",
    "            train_true.append(targets.cpu().detach().numpy())\n",
    "            \n",
    "            if i == 2000 or i == 8000:\n",
    "                opt_func.lr = opt_func.lr/3\n",
    "                opt_func.update_regularizer()\n",
    "\n",
    "        train_true = np.concatenate(train_true)\n",
    "        train_pred = np.concatenate(train_pred)\n",
    "        pickle.dump({'train_true': train_true, 'train_pred': train_pred}, open(model_dir/f'train_metrics_{epoch}.p', 'wb'))\n",
    "        train_auc = roc_auc_score(train_true, train_pred) \n",
    "        \n",
    "        # Eval model\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_true = [] \n",
    "        for j, (test_data, test_targets) in enumerate(dls.valid):\n",
    "            y_pred = model(test_data)\n",
    "            test_pred.append(y_pred.cpu().detach().numpy())\n",
    "            test_true.append(test_targets.numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        val_auc =  roc_auc_score(test_true, test_pred) \n",
    "\n",
    "        # print results\n",
    "        print(\"epoch: {}, train_loss: {:4f}, train_auc:{:4f}, test_auc:{:4f}, lr:{:4f}\".format(epoch, loss.item(), train_auc, val_auc, optimizer.lr ))\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if val_auc > max_auc:\n",
    "            max_auc = val_auc\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'best_auc': val_auc,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : opt_func.state_dict()\n",
    "            }, f\"m-epoch {epoch+1}-{time.strftime('%Y_%h_%d-%H_%M_%S')}.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_dam, dls, loss_func, opt_func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
